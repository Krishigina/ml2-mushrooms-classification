# Лабораторная работа: Классификация грибов

Этот проект представляет собой решение задачи бинарной классификации для определения, является ли гриб съедобным (`edible`) или ядовитым (`poisonous`), на основе набора данных "Secondary Mushroom Dataset".

## 1. Постановка задачи

Основная цель работы — разработать модель машинного обучения с максимально высокой точностью и надежностью.

**Приоритет безопасности:** Ключевой задачей является минимизация риска неверной классификации ядовитого гриба как съедобного (ошибка I рода, False Positive).

Проект включает в себя следующие этапы:
-   Разведочный анализ и предварительная обработка данных.
-   Построение базовой модели (baseline) на основе **Decision Tree**.
-   Применение техник работы с признаками для улучшения качества.
-   Обучение и настройка двух ансамблевых моделей: **Random Forest** и **XGBoost**.
-   Оценка моделей с использованием различных метрик, включая матрицу ошибок.
-   Анализ важности признаков для интерпретации результатов лучшей модели.

## 2. Разведочный анализ данных (EDA)

-   **Набор данных:** `secondary_data.csv`, содержащий 61,069 записей и 21 признак.
-   **Целевая переменная:** `class` ('e' - edible, 'p' - poisonous).
-   **Ключевые находки:**
    -   **Пропуски:** Обнаружено значительное количество пропущенных значений. Четыре признака имели более 80% пропусков: `veil-type` (94.8%), `spore-print-color` (89.6%), `veil-color` (87.9%) и `stem-root` (84.4%).
    -   **Дубликаты:** Выявлено 146 дублирующихся записей.
    -   **Баланс классов:** Данные хорошо сбалансированы — **55.5%** ядовитых грибов и **44.5%** съедобных. Для корректной оценки при разделении на выборки применялась стратификация.

## 3. Предотвращение утечки данных (Data Leakage)

Для обеспечения честной и объективной оценки моделей на протяжении всей работы применялись строгие меры по предотвращению утечки данных:
1.  **Раннее разделение:** Данные были разделены на обучающую и тестовую выборки в самом начале.
2.  **Обучение на `train`:** Все шаги предобработки (вычисление медиан/мод для импутации, определение категорий для кодирования, решение об удалении признаков) основывались **исключительно на обучающей выборке**.
3.  **Использование `Pipeline`:** Весь процесс предобработки был инкапсулирован в `Pipeline`, что гарантировало применение правил, выученных на `train`, для преобразования `test` без "подглядывания" в тестовые данные.

## 4. Построение моделей и результаты

Все модели обучались на обучающей выборке (80% данных) и оценивались на отложенной тестовой выборке (20% данных).

### 4.1. Базовая модель (Baseline)
-   **Модель:** `DecisionTreeClassifier`
-   **Предобработка:** Заполнение пропусков (медиана/мода), One-Hot кодирование.
-   **Результат (на тесте):** Точность (Accuracy) = **0.9985**.

### 4.2. Продвинутые ансамблевые модели

-   **Подход к признакам:** Для обеих моделей были **удалены 4 признака с более чем 80% пропущенных значений**, что позволило алгоритмам сфокусироваться на более надежных данных.

#### Random Forest
-   **Настройка:** Гиперпараметры подбирались с помощью `GridSearchCV`.
-   **Результат (на тесте):** Точность (Accuracy) = **1.0000**. Модель показала идеальный результат, не допустив ни одной ошибки.

#### XGBoost (Градиентный бустинг)
-   **Настройка:** Аналогично, `GridSearchCV` использовался для подбора гиперпараметров (`n_estimators`, `learning_rate`, `max_depth`).
-   **Результат (на тесте):** Точность (Accuracy) = **0.9998**.

## 5. Оценка и важность признаков

Финальной и лучшей моделью был выбран **Random Forest**, как первый достигший 100% точности.

-   **Матрица ошибок** подтвердила полное отсутствие ошибок на тестовой выборке. Это означает, что **не было ни одного случая**, когда ядовитый гриб был бы классифицирован как съедобный.
-   **Анализ важности признаков** показал, что ключевой вклад в принятие решения внесли **физические размеры гриба** и его ключевые морфологические характеристики:
    1.  `stem-width` (ширина ножки)
    2.  `cap-diameter` (диаметр шляпки)
    3.  `stem-height` (высота ножки)
    4.  `stem-color_w` (белый цвет ножки)
    5.  `gill-spacing_c` (скученное расположение пластинок)

## 6. Итоговый вывод

Базовая модель `Decision Tree` была успешно превзойдена за счет двух ключевых факторов:
1.  **Качественной и безопасной предобработки данных**: удаление шумовых признаков и строгое предотвращение утечек данных.
2.  **Использования более мощных ансамблевых методов** (`Random Forest` и `XGBoost`), которые более устойчивы к переобучению и лучше улавливают сложные зависимости.

В результате удалось построить модель **Random Forest**, достигающую **100% точности** на тестовой выборке. Это доказывает, что для данного симулированного набора данных задача классификации может быть решена с идеальным качеством.

## 7. Как запустить

1. Клонируйте репозиторий.
2.  Создайте и активируйте виртуальное окружение:
    ```bash
    python -m venv .venv
    # Windows
    .venv\Scripts\activate
    # macOS/Linux
    source .venv/bin/activate
    ```
3.  Установите все зависимости из файла `requirements.txt`:
    ```bash
    pip install -r requirements.txt
    ```
4.  Откройте и запустите ячейки в Jupyter Notebook `main.ipynb`